{
  "author": "Luyang Luo",
  "introduction": "My research is dedicated to advancing the intersection of artificial intelligence and healthcare, focusing on enhancing diagnostic accuracy and patient care through innovative methodologies and making AI technology more accessible to both clinicians and patients.",
  "topics": [
    {
      "name": "Foundation Models for Enhanced Medical Image Interpretation",
      "synthesis": "We have achieved significant breakthroughs in advancing foundation models for medical image interpretation across multiple domains, from breast cancer diagnostics to CT report generation. We demonstrated the power of multimodal approaches through our MOME model for breast cancer MRI analysis and our GSCo framework, which uniquely combines generalist foundation models with specialist expertise to enhance diagnostic accuracy. We innovated representation learning through our GL-MAE framework, addressing key challenges in volumetric medical image segmentation by effectively capturing both global and local features. We further expanded the capabilities of medical AI through Dia-LLaMA, showing promise in generating more natural and clinically relevant medical reports from imaging data. Through these complementary approaches, we have collectively advanced medical image interpretation by combining the strengths of various AI architectures to improve diagnostic accuracy and patient care outcomes.",
      "papers": [
        {
          "title": "Towards Non-invasive and Personalized Management of Breast Cancer Patients from Multiparametric MRI via A Large Mixture-of-Modality-Experts Model",
          "authors": [
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Mingxiang Wu",
              "normalized_name": "Mingxiang Wu"
            },
            {
              "full_name": "Mei Li",
              "normalized_name": "Mei Li"
            },
            {
              "full_name": "Yi Xin",
              "normalized_name": "Yi Xin"
            },
            {
              "full_name": "Qiong Wang",
              "normalized_name": "Qiong Wang"
            },
            {
              "full_name": "Varut Vardhanabhuti",
              "normalized_name": "Varut Vardhanabhuti"
            },
            {
              "full_name": "Winnie CW Chu",
              "normalized_name": "Winnie CW Chu"
            },
            {
              "full_name": "Zhenhui Li",
              "normalized_name": "Zhenhui Li"
            },
            {
              "full_name": "Juan Zhou",
              "normalized_name": "Juan Zhou"
            },
            {
              "full_name": "Pranav Rajpurkar",
              "normalized_name": "Pranav Rajpurkar"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the challenge of integrating multiparametric MRI data for breast cancer diagnosis, which has been limited by reliance on single sequences in AI studies. The authors introduce a large mixture-of-modality-experts model (MOME) that effectively combines multiple MRI modalities, achieving an AUROC of 0.913 and significantly reducing unnecessary biopsies for BI-RADS 4 patients. The model demonstrates comparable performance to experienced radiologists, highlighting its potential for personalized breast cancer management.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/Towards Non-invasive and Personalized Management of Breast Cancer Patients from Multiparametric MRI via A Large Mixture-of-Modality-Experts Model.pdf"
        },
        {
          "title": "GSCo: Towards Generalizable AI in Medicine via Generalist-Specialist Collaboration",
          "authors": [
            {
              "full_name": "Sunan He",
              "normalized_name": "Sunan He"
            },
            {
              "full_name": "Yuxiang Nie",
              "normalized_name": "Yuxiang Nie"
            },
            {
              "full_name": "Hongmei Wang",
              "normalized_name": "Hongmei Wang"
            },
            {
              "full_name": "Shu Yang",
              "normalized_name": "Shu Yang"
            },
            {
              "full_name": "Yihui Wang",
              "normalized_name": "Yihui Wang"
            },
            {
              "full_name": "Zhiyuan Cai",
              "normalized_name": "Zhiyuan Cai"
            },
            {
              "full_name": "Zhixuan Chen",
              "normalized_name": "Zhixuan Chen"
            },
            {
              "full_name": "Yingxue Xu",
              "normalized_name": "Yingxue Xu"
            },
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Huiling Xiang",
              "normalized_name": "Huiling Xiang"
            },
            {
              "full_name": "Xi Lin",
              "normalized_name": "Xi Lin"
            },
            {
              "full_name": "Mingxiang Wu",
              "normalized_name": "Mingxiang Wu"
            },
            {
              "full_name": "Yifan Peng",
              "normalized_name": "Yifan Peng"
            },
            {
              "full_name": "George Shih",
              "normalized_name": "George Shih"
            },
            {
              "full_name": "Ziyang Xu",
              "normalized_name": "Ziyang Xu"
            },
            {
              "full_name": "Xian Wu",
              "normalized_name": "Xian Wu"
            },
            {
              "full_name": "Qiong Wang",
              "normalized_name": "Qiong Wang"
            },
            {
              "full_name": "Ronald Cheong Kin Chan",
              "normalized_name": "Ronald Chan"
            },
            {
              "full_name": "Varut Vardhanabhuti",
              "normalized_name": "Varut Vardhanabhuti"
            },
            {
              "full_name": "Winnie Chiu Wing Chu",
              "normalized_name": "Winnie Chu"
            },
            {
              "full_name": "Yefeng Zheng",
              "normalized_name": "Yefeng Zheng"
            },
            {
              "full_name": "Pranav Rajpurkar",
              "normalized_name": "Pranav Rajpurkar"
            },
            {
              "full_name": "Kang Zhang",
              "normalized_name": "Kang Zhang"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the challenge of achieving precise medical image analysis by leveraging both Generalist Foundation Models (GFMs) and specialist models. The key innovation is the Generalist-Specialist Collaboration (GSCo) framework, which integrates GFMs' generalizability with the precision of specialist models through cooperative mechanisms like Mixture-of-Expert Diagnosis (MoED) and Retrieval-Augmented Diagnosis (RAD). Empirical results demonstrate that the proposed framework significantly enhances diagnostic accuracy across diverse medical tasks, outperforming both GFMs and specialist models in various scenarios.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/GSCo- Towards Generalizable AI in Medicine via Generalist-Specialist Collaboration.pdf"
        },
        {
          "title": "Advancing Volumetric Medical Image Segmentation via Global-Local Masked Autoencoder",
          "authors": [
            {
              "full_name": "Jia-Xin Zhuang",
              "normalized_name": "Jia Xin Zhuang"
            },
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the challenges of applying masked autoencoders (MAE) to volumetric medical images, specifically the lack of global context and instability in learned representations. The authors propose the Global-Local Masked AutoEncoder (GL-MAE), which enhances representation learning by reconstructing both masked global and local views, leading to improved performance in volumetric medical image segmentation tasks. Empirical results demonstrate that GL-MAE outperforms state-of-the-art methods, achieving significant improvements in segmentation accuracy across multiple datasets.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/Advancing Volumetric Medical Image Segmentation via Global-Local Masked.pdf"
        },
        {
          "title": "Dia-LLaMA: Towards Large Language Model-driven CT Report Generation",
          "authors": [
            {
              "full_name": "Zhixuan Chen",
              "normalized_name": "Zhixuan Chen"
            },
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Yequan Bie",
              "normalized_name": "Yequan Bie"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the challenges in automated CT report generation, particularly the data imbalance and the rigid template structure of reports. The authors introduce Dia-LLaMA, a framework that utilizes LLaMA2-7B and incorporates diagnostic prompts and a disease prototype memory bank to enhance report generation. Experimental results demonstrate that Dia-LLaMA outperforms existing methods, achieving state-of-the-art performance in clinical efficacy and natural language generation metrics.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/Dia-LLaMA- Towards Large Language Model-driven CT Report Generation.pdf"
        }
      ]
    },
    {
      "name": "Explain Healthcare with AI and Explainable AI for Healthcare",
      "synthesis": "We have pioneered multiple approaches to make healthcare AI more transparent and accessible to both clinicians and patients through our innovative solutions. We developed ReXplain, a groundbreaking system that transforms complex radiology reports into engaging video presentations with simplified language and visual aids, directly addressing patient comprehension challenges. We advanced explainable skin lesion diagnosis through our MICA framework, which uniquely aligns medical images with clinical concepts at multiple levels, while our XCoOp system enhances prompt learning interpretability in medical image analysis. We conducted comprehensive research through our survey on Self-eXplainable AI (S-XAI), identifying the critical need for inherent explainability in medical AI systems during the training process. Through these complementary approaches, we have established new standards for explainable and patient-centered AI in healthcare, improving both diagnostic transparency and patient engagement.",
      "papers": [
        {
          "title": "ReXplain: Translating Radiology into Patient-Friendly Video Reports",
          "authors": [
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Jenanan Vairavamurthy",
              "normalized_name": "Jenanan Vairavamurthy"
            },
            {
              "full_name": "Xiaoman Zhang",
              "normalized_name": "Xiaoman Zhang"
            },
            {
              "full_name": "Abhinav Kumar",
              "normalized_name": "Abhinav Kumar"
            },
            {
              "full_name": "Ramon R. Ter-Oganesyan",
              "normalized_name": "Ramon Ter-Oganesyan"
            },
            {
              "full_name": "Stuart T. Schroff",
              "normalized_name": "Stuart Schroff"
            },
            {
              "full_name": "Dan Shilo",
              "normalized_name": "Dan Shilo"
            },
            {
              "full_name": "Rydhwana Hossain",
              "normalized_name": "Rydhwana Hossain"
            },
            {
              "full_name": "Mike Moritz",
              "normalized_name": "Mike Moritz"
            },
            {
              "full_name": "Pranav Rajpurkar",
              "normalized_name": "Pranav Rajpurkar"
            }
          ],
          "summary": "This paper addresses the problem of radiology reports being incomprehensible to patients, which can lead to anxiety and poor health outcomes. The key innovation is the ReXplain system, which uses AI to translate complex radiology findings into engaging video reports that combine simplified language, anatomical imagery, and 3D renderings. Empirical findings from user feedback indicate that ReXplain effectively communicates radiological information, enhancing patient understanding and engagement.",
          "weight": 0.85,
          "role": "primary_research",
          "file_path": "pdfs/ReXplain- Translating Radiology into Patient-Friendly Video Reports.pdf"
        },
        {
          "title": "MICA: Towards Explainable Skin Lesion Diagnosis via Multi-Level Image-Concept Alignment",
          "authors": [
            {
              "full_name": "Yequan Bie",
              "normalized_name": "Yequan Bie"
            },
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the challenge of achieving explainable skin lesion diagnosis using deep learning methods, which often lack transparency. The authors propose a novel framework, MICA, that aligns medical images with clinical concepts at multiple levels (image, token, and concept levels) to enhance interpretability while maintaining high diagnostic performance. Experimental results demonstrate that MICA outperforms existing methods in concept detection and disease diagnosis across multiple datasets, achieving superior accuracy and label efficiency.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/[AAAI 2024] MICA-Towards Explainable Skin Lesion Diagnosis via Multi-Level Image-Concept Alignment.pdf"
        },
        {
          "title": "XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization",
          "authors": [
            {
              "full_name": "Yequan Bie",
              "normalized_name": "Yequan Bie"
            },
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Zhixuan Chen",
              "normalized_name": "Zhixuan Chen"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the challenge of explainability in prompt learning methods for medical image analysis, particularly in high-stakes healthcare scenarios. The authors propose XCoOp, a novel framework that aligns medical knowledge with learnable prompts and clinical concepts at multiple granularities, enhancing both interpretability and diagnostic performance. Empirical results demonstrate that XCoOp significantly outperforms existing methods, achieving notable improvements in diagnostic accuracy across various datasets.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/XCoOp- Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization.pdf"
        },
        {
          "title": "Self-eXplainable AI for Medical Image Analysis: A Survey and New Outlooks",
          "authors": [
            {
              "full_name": "Junlin Hou",
              "normalized_name": "Junlin Hou"
            },
            {
              "full_name": "Sicen Liu",
              "normalized_name": "Sicen Liu"
            },
            {
              "full_name": "Y equan Bie",
              "normalized_name": "Y equan Bie"
            },
            {
              "full_name": "Hongmei Wang",
              "normalized_name": "Hongmei Wang"
            },
            {
              "full_name": "Andong Tan",
              "normalized_name": "Andong Tan"
            },
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the challenge of explainability in AI models used for medical image analysis, proposing Self-eXplainable AI (S-XAI) as a solution. The key innovation is the integration of explainability into the training process of deep learning models, allowing them to generate inherent explanations aligned with their decision-making. The survey reviews over 200 papers, highlighting the growing importance of S-XAI in enhancing the transparency and trustworthiness of AI systems in clinical applications, with empirical findings indicating a significant increase in research interest in this area.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/Self-eXplainable AI for Medical Image Analysis- A Survey and New Outlooks.pdf"
        }
      ]
    },
    {
      "name": "Multimodal Learning Models Enhance Cancer Diagnosis",
      "synthesis": "We have made remarkable advances in cancer diagnosis through our development of innovative multimodal learning approaches that integrate diverse data sources. We conducted a comprehensive review of deep learning applications in breast cancer imaging across multiple modalities, demonstrating how AI models can achieve performance comparable to expert radiologists. We developed OvcaFinder, an interpretable model that significantly improves ovarian cancer diagnosis by combining ultrasound images, radiologist assessments, and clinical variables, achieving remarkable AUC scores and reducing false positive rates. We extensively reviewed multimodal data integration techniques in precision oncology through approximately 300 studies, identifying key methodological innovations and their impact on cancer care. Through these complementary approaches, we have established new frameworks for cancer diagnosis that leverage the power of multimodal integration to enhance clinical decision-making and improve patient outcomes.",
      "papers": [
        {
          "title": "Deep Learning in Breast Cancer Imaging: A Decade of Progress and Future Directions",
          "authors": [
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Xi Wang",
              "normalized_name": "Xi Wang"
            },
            {
              "full_name": "Yi Lin",
              "normalized_name": "Yi Lin"
            },
            {
              "full_name": "Xiaoqi Ma",
              "normalized_name": "Xiaoqi Ma"
            },
            {
              "full_name": "Andong Tan",
              "normalized_name": "Andong Tan"
            },
            {
              "full_name": "Ronald Chan",
              "normalized_name": "Ronald Chan"
            },
            {
              "full_name": "Varut Vardhanabhuti",
              "normalized_name": "Varut Vardhanabhuti"
            },
            {
              "full_name": "Winnie CW Chu",
              "normalized_name": "Winnie CW Chu"
            },
            {
              "full_name": "Kwang-Ting Cheng",
              "normalized_name": "Kwang-Ting Cheng"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the critical issue of breast cancer diagnosis and treatment through deep learning techniques applied to various imaging modalities. The key innovation lies in the comprehensive review of deep learning applications across mammography, ultrasound, MRI, and digital pathology, highlighting advancements in screening, diagnosis, and prognosis. Empirical findings indicate that deep learning models have significantly improved diagnostic accuracy, with studies showing performance comparable to expert radiologists, thus underscoring the potential for enhanced clinical outcomes.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/[IEEE RBME 2024] Deep_Learning_in_Breast_Cancer_Imaging_A_Decade_of_Progress_and_Future_Directions.pdf"
        },
        {
          "title": "Development and validation of an interpretable model integrating multimodal information for improving ovarian cancer diagnosis",
          "authors": [
            {
              "full_name": "Huiling Xiang",
              "normalized_name": "Huiling Xiang"
            },
            {
              "full_name": "Yongjie Xiao",
              "normalized_name": "Yongjie Xiao"
            },
            {
              "full_name": "Fang Li",
              "normalized_name": "Fang Li"
            },
            {
              "full_name": "Chunyan Li",
              "normalized_name": "Chunyan Li"
            },
            {
              "full_name": "Lixian Liu",
              "normalized_name": "Lixian Liu"
            },
            {
              "full_name": "Tingting Deng",
              "normalized_name": "Tingting Deng"
            },
            {
              "full_name": "Cuiju Yan",
              "normalized_name": "Cuiju Yan"
            },
            {
              "full_name": "Fengtao Zhou",
              "normalized_name": "Fengtao Zhou"
            },
            {
              "full_name": "Xi Wang",
              "normalized_name": "Xi Wang"
            },
            {
              "full_name": "Jinjing Ou",
              "normalized_name": "Jinjing Ou"
            },
            {
              "full_name": "Qingguang Lin",
              "normalized_name": "Qingguang Lin"
            },
            {
              "full_name": "Ruixia Hong",
              "normalized_name": "Ruixia Hong"
            },
            {
              "full_name": "Lishu Huang",
              "normalized_name": "Lishu Huang"
            },
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Huangjing Lin",
              "normalized_name": "Huangjing Lin"
            },
            {
              "full_name": "Xi Lin",
              "normalized_name": "Xi Lin"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This study addresses the critical issue of accurate and early diagnosis of ovarian cancer, which is essential for improving patient outcomes. The key innovation is the development of OvcaFinder, an interpretable model that integrates ultrasound images, radiologist assessments, and clinical variables, achieving AUCs of 0.978 and 0.947 in internal and external test datasets, respectively. The model significantly enhances diagnostic accuracy and consistency among radiologists, reducing false positive rates by 13.4% and 8.3% in the respective datasets.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/[Nature Communications 2024] Development and validation of an interpretable model integrating multimodal information for improving ovarian cancer diagnosis.pdf"
        },
        {
          "title": "Multimodal Data Integration for Precision Oncology: Challenges and Future Directions",
          "authors": [
            {
              "full_name": "Huajun Zhou",
              "normalized_name": "Huajun Zhou"
            },
            {
              "full_name": "Fengtao Zhou",
              "normalized_name": "Fengtao Zhou"
            },
            {
              "full_name": "Chenyu Zhao",
              "normalized_name": "Chenyu Zhao"
            },
            {
              "full_name": "Yingxue Xu",
              "normalized_name": "Yingxue Xu"
            },
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the challenge of integrating diverse data modalities in precision oncology to enhance clinical decision-making and treatment outcomes. The authors present a comprehensive review of approximately 300 studies on multimodal data integration techniques, highlighting key innovations in methodologies and their applications in cancer care. The findings indicate significant advancements in early assessment, diagnosis, prognosis, and biomarker discovery, while also identifying critical challenges and future research directions in the field.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/Multimodal Data Integration for Precision Oncology- Challenges and Future Directions.pdf"
        }
      ]
    },
    {
      "name": "Robust Learning for Trustworthy Medical AI",
      "synthesis": "We have developed multiple innovative approaches to address critical challenges in building trustworthy medical AI systems through robust and responsible learning methods. We introduced Ada-ABC, a novel debiasing framework that effectively mitigates dataset bias in medical image classification by learning from a biased council of classifiers without requiring explicit bias labels. We created the UMA-Net framework to tackle the challenge of noisy multi-source annotations in medical image segmentation through uncertainty-guided learning mechanisms. We pioneered the Federated Client Unlearning framework to enable efficient data removal in federated learning systems while preserving model performance, addressing crucial privacy concerns in medical imaging. Through our scale-aware super-resolution network with dual affinity learning, we significantly improved lesion segmentation accuracy across varying sizes and resolutions, collectively advancing the robustness and reliability of medical AI systems.",
      "papers": [
        {
          "title": "Medical Image Debiasing by Learning Adaptive Agreement from a Biased Council",
          "authors": [
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Xin Huang",
              "normalized_name": "Xin Huang"
            },
            {
              "full_name": "Minghao Wang",
              "normalized_name": "Minghao Wang"
            },
            {
              "full_name": "Zhuoyue Wan",
              "normalized_name": "Zhuoyue Wan"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the problem of dataset bias in medical image classification, which can lead to inaccurate and unfair models. The authors propose a novel debiasing framework called Adaptive Agreement from a Biased Council (Ada-ABC), which learns from a biased council of classifiers without relying on explicit bias labels. Empirical results demonstrate that Ada-ABC outperforms existing methods in mitigating dataset bias across various medical imaging scenarios, achieving significant improvements in classification accuracy.",
          "weight": 0.85,
          "role": "primary_research",
          "file_path": "pdfs/[arxiv 2024] Medical Image Debiasing by Learning Adaptive Agreement from a Biased Council.pdf"
        },
        {
          "title": "Learning Robust Medical Image Segmentation from Multi-source Annotations",
          "authors": [
            {
              "full_name": "Yifeng Wang",
              "normalized_name": "Yifeng Wang"
            },
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Mingxiang Wu",
              "normalized_name": "Mingxiang Wu"
            },
            {
              "full_name": "Qiong Wang",
              "normalized_name": "Qiong Wang"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the challenge of learning segmentation networks from multi-source annotations in medical imaging, which often suffer from noise and bias. The authors introduce the Uncertainty-guided Multi-source Annotation Network (UMA-Net), which employs an Annotation Uncertainty Estimation Module (AUEM) and a Quality Assessment Module (QAM) to guide the training process by estimating pixel-wise and image-level uncertainties. Extensive experiments demonstrate that UMA-Net achieves state-of-the-art performance across various medical imaging datasets, effectively leveraging diverse annotations.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/Learning Robust Medical Image Segmentation from Multi-source Annotations.pdf"
        },
        {
          "title": "Enable the Right to be Forgotten with Federated Client Unlearning in Medical Imaging",
          "authors": [
            {
              "full_name": "Zhipeng Deng",
              "normalized_name": "Zhipeng Deng"
            },
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the challenge of implementing the right to be forgotten in federated learning, particularly in medical imaging. The authors introduce a novel Federated Client Unlearning (FCU) framework that utilizes Model-Contrastive Unlearning and Frequency-Guided Memory Preservation to efficiently erase client data contributions while maintaining model performance. Empirical results demonstrate that the FCU framework achieves a 10-15 times speed-up compared to traditional retraining methods, effectively balancing efficiency and privacy.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/Enable the Right to be Forgotten with Federated Client Unlearning in Medical Imaging.pdf"
        },
        {
          "title": "Scale-aware Super-resolution Network with Dual Affinity Learning for Lesion Segmentation from Medical Images",
          "authors": [
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Yanwen Li",
              "normalized_name": "Yanwen Li"
            },
            {
              "full_name": "Zhizhong Chai",
              "normalized_name": "Zhizhong Chai"
            },
            {
              "full_name": "Huangjing Lin",
              "normalized_name": "Huangjing Lin"
            },
            {
              "full_name": "Pheng-Ann Heng",
              "normalized_name": "Pheng-Ann Heng"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the challenge of lesion segmentation in medical images, particularly due to the variability in lesion sizes and low image resolutions. The authors propose a novel scale-aware super-resolution network that utilizes dual branches for both lesion mask and image super-resolution, incorporating scale-aware dilated convolution blocks to dynamically adjust receptive fields based on lesion sizes. Empirical results demonstrate significant improvements in segmentation performance across multiple datasets, achieving state-of-the-art results compared to existing methods.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/[TNNLS 2024] Scale-aware Super-resolution Network with Dual Affinity Learning for Lesion Segmentation from Medical Images.pdf"
        }
      ]
    },
    {
      "name": "Transformative Approaches to Computational Pathology and Surgical Phase Recognition",
      "synthesis": "We have pioneered transformative approaches in computational pathology and surgical phase recognition through multiple innovative frameworks. We developed two groundbreaking solutions for whole slide image classification: our HMIL framework that leverages hierarchical label correlations for fine-grained analysis, and our ICMIL method that efficiently couples bag-level classifiers with patch embedders. We revolutionized surgical phase recognition through our development of SurgPETL, which efficiently adapts image-level pre-trained models through spatial-temporal adaptation, and Surgformer, which employs hierarchical temporal attention for enhanced temporal understanding. We designed these frameworks to address critical challenges in both fields, with HMIL and ICMIL tackling the complexities of cancer subtype classification, while SurgPETL and Surgformer focus on robust surgical workflow analysis. Through comprehensive empirical validation, our approaches have demonstrated state-of-the-art performance across multiple datasets, significantly advancing both computational pathology and surgical phase recognition.",
      "papers": [
        {
          "title": "HMIL: Hierarchical Multi-Instance Learning for Fine-Grained Whole Slide Image Classification",
          "authors": [
            {
              "full_name": "Cheng Jin",
              "normalized_name": "Cheng Jin"
            },
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Huangjing Lin",
              "normalized_name": "Huangjing Lin"
            },
            {
              "full_name": "Jun Hou",
              "normalized_name": "Jun Hou"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the challenge of fine-grained classification of whole slide images (WSIs) in precision oncology, where subtle morphological variations among cancer subtypes must be distinguished. The authors propose a novel hierarchical multi-instance learning (HMIL) framework that incorporates hierarchical label correlations and a class-wise attention mechanism to enhance the learning process. Empirical results demonstrate that HMIL achieves state-of-the-art performance on multiple datasets, significantly improving classification accuracy and sensitivity compared to existing methods.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/HMIL_Hierarchical_Multi-Instance_Learning_for_Fine-Grained_Whole_Slide_Image_Classification.pdf"
        },
        {
          "title": "Rethinking Multiple Instance Learning for Whole Slide Image Classification: A Bag-Level Classifier is a Good Instance-Level Teacher",
          "authors": [
            {
              "full_name": "Hongyi Wang",
              "normalized_name": "Hongyi Wang"
            },
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Fang Wang",
              "normalized_name": "Fang Wang"
            },
            {
              "full_name": "Ruofeng Tong",
              "normalized_name": "Ruofeng Tong"
            },
            {
              "full_name": "Y en-Wei Chen",
              "normalized_name": "Yen-Wei Chen"
            },
            {
              "full_name": "Hongjie Hu",
              "normalized_name": "Hongjie Hu"
            },
            {
              "full_name": "Lanfen Lin",
              "normalized_name": "Lanfen Lin"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the challenge of high computational costs in Whole Slide Image (WSI) classification using Multiple Instance Learning (MIL). The authors propose Iteratively Coupled Multiple Instance Learning (ICMIL), which iteratively couples a bag-level classifier with a patch embedder to improve classification accuracy. Empirical results demonstrate that ICMIL significantly enhances the performance of existing MIL methods, achieving state-of-the-art results across multiple datasets.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/[TMI 2024] Rethinking Multiple Instance Learning for Whole Slide Image Classification- A Bag-Level Classifier is a Good Instance-Level Teacher.pdf"
        },
        {
          "title": "SurgPETL: Parameter-Efficient Image-to-Surgical-Video Transfer Learning for Surgical Phase Recognition",
          "authors": [
            {
              "full_name": "Shu Yang",
              "normalized_name": "Shu Yang"
            },
            {
              "full_name": "Zhiyuan Cai",
              "normalized_name": "Zhiyuan Cai"
            },
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Ning Ma",
              "normalized_name": "Ning Ma"
            },
            {
              "full_name": "Shuchang Xu",
              "normalized_name": "Shuchang Xu"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the challenge of efficiently adapting image-level pre-trained models for fine-grained surgical phase recognition, which is hindered by limited video data and the need for robust spatial-temporal modeling. The authors introduce a novel framework, SurgPETL, which incorporates a Spatial-Temporal Adaptation (STA) module to enhance feature extraction across spatial and temporal dimensions. Extensive experiments demonstrate that SurgPETL-STA outperforms existing methods while maintaining parameter efficiency, achieving significant improvements in accuracy across multiple surgical datasets.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/SurgPETL- Parameter-Efficient Image-to-Surgical-Video Transfer Learning for Surgical Phase Recognition.pdf"
        },
        {
          "title": "Surgformer: Surgical Transformer with Hierarchical Temporal Attention for Surgical Phase Recognition",
          "authors": [
            {
              "full_name": "Shu Yang",
              "normalized_name": "Shu Yang"
            },
            {
              "full_name": "Luyang Luo",
              "normalized_name": "Luyang Luo"
            },
            {
              "full_name": "Qiong Wang",
              "normalized_name": "Qiong Wang"
            },
            {
              "full_name": "Hao Chen",
              "normalized_name": "Hao Chen"
            }
          ],
          "summary": "This paper addresses the limitations of existing surgical phase recognition methods, which struggle with spatial-temporal dependency and redundancy. The authors introduce the Surgical Transformer (Surgformer), which utilizes a novel Hierarchical Temporal Attention (HTA) mechanism to effectively capture both global and local information across varying temporal resolutions. Empirical results demonstrate that Surgformer outperforms state-of-the-art methods on benchmark datasets, achieving significant improvements in accuracy and F1 scores.",
          "weight": 0.9,
          "role": "primary_research",
          "file_path": "pdfs/Surgformer- Surgical Transformer with Hierarchical Temporal Attention for Surgical Phase Recognition.pdf"
        }
      ]
    }
  ]
}
